{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIpSJMe5f8rv+IMKE9VnVY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashrith3456/-report-on-the-impact-of-Generative-AI-on-software-development-and-low-code-no-code-platforms.-/blob/main/AI_Literature_Review_Assistant_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, os\n",
        "\n",
        "# Make sure NLTK has a place to store data\n",
        "os.makedirs(\"/root/nltk_data\", exist_ok=True)\n",
        "nltk.data.path.append(\"/root/nltk_data\")\n",
        "\n",
        "# Download both (some environments require punkt_tab too)\n",
        "nltk.download(\"punkt\")\n",
        "try:\n",
        "    nltk.download(\"punkt_tab\")\n",
        "except Exception as e:\n",
        "    print(\"punkt_tab not available in this NLTK build; safe to ignore.\")\n",
        "\n",
        "print(\"âœ… NLTK tokenizers ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBOMT2To4Evi",
        "outputId": "c83e49dc-4551-4a0b-be1e-64d37dfa1a62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… NLTK tokenizers ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === FINAL ONE-CELL LITERATURE REVIEW ASSISTANT (no API needed) ===\n",
        "# Install deps (and pin requests to avoid Colab conflicts)\n",
        "with open(\"constraints.txt\",\"w\") as f: f.write(\"requests==2.32.4\\n\")\n",
        "!pip -q install -c constraints.txt pypdf nltk sumy sentence-transformers\n",
        "\n",
        "# Imports & setup\n",
        "import os, re\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "os.makedirs(\"pdfs\", exist_ok=True)\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "# --- Ensure we have paper1.pdf & paper2.pdf ---\n",
        "have1 = os.path.exists(\"pdfs/paper1.pdf\")\n",
        "have2 = os.path.exists(\"pdfs/paper2.pdf\")\n",
        "if not (have1 and have2):\n",
        "    print(\"ðŸ“¥ Upload two PDFs (any names). Theyâ€™ll be saved as paper1.pdf & paper2.pdf\")\n",
        "    uploaded = files.upload()  # pick two files\n",
        "    picked = [n for n in uploaded.keys() if n.lower().endswith(\".pdf\")]\n",
        "    assert len(picked) >= 2, \"Please upload at least TWO PDF files.\"\n",
        "    os.rename(picked[0], \"pdfs/paper1.pdf\")\n",
        "    os.rename(picked[1], \"pdfs/paper2.pdf\")\n",
        "\n",
        "# --- Helpers (extract, summarize, keywords, compare, gaps) ---\n",
        "from pypdf import PdfReader\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "def extract_text(pdf_path, max_pages=12):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = reader.pages[:max_pages]\n",
        "    return \"\\n\".join(p.extract_text() or \"\" for p in pages)\n",
        "\n",
        "def summarize_text(text, n_sentences=8):\n",
        "    text = (text or \"\").strip()\n",
        "    if not text: return \"No content found.\"\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    sents = LexRankSummarizer()(parser.document, n_sentences)\n",
        "    return \"\\n\".join(str(s) for s in sents)\n",
        "\n",
        "def extract_keywords(text, top_k=15):\n",
        "    words = [w.lower() for w in re.findall(r\"[A-Za-z][A-Za-z\\-]{2,}\", text or \"\")]\n",
        "    stop = set(\"\"\"the a an and or of in on for to from with by as that this these those is are was were be being been it its\n",
        "    at we you they he she him her their our your not into over under out more less most least about using use\n",
        "    method methods approach approaches result results data model models paper study studies system systems\"\"\".split())\n",
        "    cand = [w for w in words if w not in stop]\n",
        "    return [w for w,_ in Counter(cand).most_common(top_k)]\n",
        "\n",
        "def compare_summaries(sum1, sum2):\n",
        "    set1, set2 = set(sum1.lower().split()), set(sum2.lower().split())\n",
        "    overlap = \" \".join(list((set1 & set2))[:30])\n",
        "    only1 = \" \".join(list(set1 - set2)[:30])\n",
        "    only2 = \" \".join(list(set2 - set1)[:30])  # <-- FIXED HERE\n",
        "    return overlap, only1, only2\n",
        "\n",
        "def infer_gaps(*texts):\n",
        "    joined = \"\\n\".join(texts)\n",
        "    kws = extract_keywords(joined, 12)\n",
        "    focus = \", \".join(kws[:6]) if kws else \"the topic\"\n",
        "    return f\"\"\"- Limited standardized benchmarks & head-to-head comparisons\n",
        "- Weak generalization across datasets/projects; need cross-domain evaluation\n",
        "- Sparse error analysis & failure taxonomy\n",
        "- Reproducibility issues (datasets, seeds, exact configs)\n",
        "- Limited reporting on computational cost / flakiness\n",
        "Focus cues: {focus}\"\"\"\n",
        "\n",
        "# --- Process both PDFs ---\n",
        "P1, P2 = \"pdfs/paper1.pdf\", \"pdfs/paper2.pdf\"\n",
        "t1, t2 = extract_text(P1), extract_text(P2)\n",
        "s1, s2 = summarize_text(t1), summarize_text(t2)\n",
        "k1, k2 = extract_keywords(t1), extract_keywords(t2)\n",
        "overlap, u1, u2 = compare_summaries(s1, s2)\n",
        "gaps = infer_gaps(s1, s2)\n",
        "\n",
        "# --- Build report + download ---\n",
        "report = f\"\"\"# Literature Review Assistant â€“ Report\n",
        "\n",
        "## Papers\n",
        "- **Paper 1:** {P1}\n",
        "- **Paper 2:** {P2}\n",
        "\n",
        "---\n",
        "\n",
        "## Summary â€“ Paper 1\n",
        "{s1}\n",
        "\n",
        "**Keywords:** {\", \".join(k1)}\n",
        "\n",
        "---\n",
        "\n",
        "## Summary â€“ Paper 2\n",
        "{s2}\n",
        "\n",
        "**Keywords:** {\", \".join(k2)}\n",
        "\n",
        "---\n",
        "\n",
        "## Comparison (token overlap heuristic)\n",
        "- **Overlap (examples):** {overlap}\n",
        "- **Unique to Paper 1:** {u1}\n",
        "- **Unique to Paper 2:** {u2}\n",
        "\n",
        "---\n",
        "\n",
        "## Research Gaps (heuristic)\n",
        "{gaps}\n",
        "\"\"\"\n",
        "out_path = \"outputs/lit_review_report.md\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"âœ… Done. Downloading reportâ€¦\")\n",
        "files.download(out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "i70Ds4kl2esW",
        "outputId": "4877a21a-ae6b-46cd-e524-5171090488b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done. Downloading reportâ€¦\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_90be570d-cc88-41eb-a5ce-d585b683cba9\", \"lit_review_report.md\", 5935)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}